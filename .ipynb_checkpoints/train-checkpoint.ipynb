{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-337ac2a50b46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGCN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from pygcn.utils import load_data, accuracy\n",
    "from pygcn.models import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "# 禁用CUDA训练\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "# 在训练通过期间验证\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "# 随机种子\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "# 要训练的epoch数\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "# 最初的学习率\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "# 权重衰减（参数L2损失）\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "# 隐藏层单元数量\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "# dropout率（1-保持概率)\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(args=[])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# 产生随机种子，以使得结果是确定的\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于随机种子这里做一些说明：众所周知，所谓随机数其实是伪随机数，所谓的‘伪’，意思是这些数其实是有规律的，只不过因为算法规律太复杂，很难看出来。所谓巧妇难为无米之炊，再厉害的算法，没有一个初始值，它也不可能凭空造出一系列随机数来，我们说的种子就是这个初始值。random随机数是这样生成的：我们将这套复杂的算法（是叫随机数生成器吧）看成一个黑盒，把我们准备好的种子扔进去，它会返给你两个东西，一个是你想要的随机数，另一个是保证能生成下一个随机数的新的种子，把新的种子放进黑盒，又得到一个新的随机数和一个新的种子，从此在生成随机数的路上越走越远。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/cora/cora.content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-93e0ca6ff5cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# idx_val: 验证集索引列表\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# idx_test: 测试集索引列表\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0madj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\MyPaperPrepare\\GraphEmbedding\\GCN\\pygcn-master-jupyter\\pygcn\\utils.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(path, dataset)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# feature为第二列到倒数第二列，labels为最后一列\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# idx_features_labels将cora数据集中的数据读取出来\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0midx_features_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}{}.content\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;31m# 存储csr型稀疏矩阵，idx_features_labels[:, 1:-1]表示取所有行，第一列到倒数第二列\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx_features_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[0;32m   1770\u001b[0m             \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1771\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1772\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1773\u001b[0m             \u001b[0mfid_ctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"+\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m             return _file_openers[ext](found, mode=mode,\n\u001b[1;32m--> 621\u001b[1;33m                                       encoding=encoding, newline=newline)\n\u001b[0m\u001b[0;32m    622\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s not found.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/cora/cora.content'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# 加载数据\n",
    "# adj: adj样本关系的对称邻接矩阵的稀疏张量\n",
    "# features: 样本特征张量\n",
    "# labels: 样本标签\n",
    "# idx_train: 训练集索引列表\n",
    "# idx_val: 验证集索引列表\n",
    "# idx_test: 测试集索引列表\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-fbca061246f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0madj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'adj' is not defined"
     ]
    }
   ],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-224628ecf868>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-0a1df294b701>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'idx_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-8f03f4e8e98f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0midx_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'idx_train' is not defined"
     ]
    }
   ],
   "source": [
    "idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'idx_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0504ff46e9f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0midx_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'idx_val' is not defined"
     ]
    }
   ],
   "source": [
    "idx_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'idx_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-51abace5a3da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0midx_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'idx_test' is not defined"
     ]
    }
   ],
   "source": [
    "idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "# 模型和优化器\n",
    "\n",
    "# GCN模型\n",
    "# nfeat输入单元数，shape[1]表示特征矩阵的维度数（列数）\n",
    "# nhid中间层单元数量\n",
    "# nclass输出单元数，即样本标签数=样本标签最大值+1\n",
    "# dropout参数\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "\n",
    "# 构造一个优化器对象Optimizer，用来保存当前的状态，并能够根据计算得到的梯度来更新参数\n",
    "# Adam优化器\n",
    "# lr学习率\n",
    "# weight_decay权重衰减（L2惩罚）\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果使用GPU则执行这里，数据写入cuda，便于后续加速\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练函数\n",
    "def train(epoch):\n",
    "    # 返回当前时间\n",
    "    t = time.time()\n",
    "    \n",
    "    # train的时候使用dropout, 测试的时候不使用dropout\n",
    "    # pytorch里面eval()固定整个网络参数，没有dropout\n",
    "    \n",
    "    # 固定语句，主要针对启用BatchNormalization和Dropout\n",
    "    model.train()\n",
    "    \n",
    "    # 把梯度置零，也就是把loss关于weight的导数变成0\n",
    "    optimizer.zero_grad()\n",
    "    # 执行GCN中的forward前向传播\n",
    "    output = model(features, adj)\n",
    "    # 最大似然/log似然损失函数，idx_train是140(0~139)\n",
    "    # nll_loss: negative log likelihood loss\n",
    "    # https://www.cnblogs.com/marsggbo/p/10401215.html\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    # 准确率\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    # 反向传播\n",
    "    loss_train.backward()\n",
    "    # 梯度下降，更新值\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate validation set performance separately,\n",
    "    # deactivates dropout during validation run.\n",
    "    # 是否在训练期间进行验证\n",
    "    if not args.fastmode:\n",
    "        # 固定语句，主要针对不启用BatchNormalization和Dropout\n",
    "        model.eval()\n",
    "        # 前向传播\n",
    "        output = model(features, adj)\n",
    "    \n",
    "    # 最大似然/log似然损失函数，idx_val是300(200~499)\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    # 准确率\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    # 正在迭代的epoch数\n",
    "    # 训练集损失函数值\n",
    "    # 训练集准确率\n",
    "    # 验证集损失函数值\n",
    "    # 验证集准确率\n",
    "    # 运行时间\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义测试函数，相当于对已有的模型在测试集上运行对应的loss与accuracy\n",
    "def test():\n",
    "    # 固定语句，主要针对不启用BatchNormalization和Dropout\n",
    "    model.eval()\n",
    "    # 前向传播\n",
    "    output = model(features, adj)\n",
    "    # 最大似然/log似然损失函数，idx_test是1000(500~1499)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    # 准确率\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    \n",
    "    # 测试集损失函数值\n",
    "    # 测试集的准确率\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 2.0030 acc_train: 0.0786 loss_val: 2.0052 acc_val: 0.1267 time: 0.0419s\n",
      "Epoch: 0002 loss_train: 1.9845 acc_train: 0.1429 loss_val: 1.9909 acc_val: 0.1267 time: 0.0170s\n",
      "Epoch: 0003 loss_train: 1.9743 acc_train: 0.1429 loss_val: 1.9770 acc_val: 0.1267 time: 0.0125s\n",
      "Epoch: 0004 loss_train: 1.9563 acc_train: 0.1357 loss_val: 1.9634 acc_val: 0.1267 time: 0.0120s\n",
      "Epoch: 0005 loss_train: 1.9477 acc_train: 0.1357 loss_val: 1.9499 acc_val: 0.1267 time: 0.0128s\n",
      "Epoch: 0006 loss_train: 1.9252 acc_train: 0.1643 loss_val: 1.9365 acc_val: 0.1500 time: 0.0123s\n",
      "Epoch: 0007 loss_train: 1.9244 acc_train: 0.2214 loss_val: 1.9234 acc_val: 0.1633 time: 0.0104s\n",
      "Epoch: 0008 loss_train: 1.8976 acc_train: 0.2286 loss_val: 1.9104 acc_val: 0.1567 time: 0.0120s\n",
      "Epoch: 0009 loss_train: 1.8902 acc_train: 0.2071 loss_val: 1.8975 acc_val: 0.1567 time: 0.0112s\n",
      "Epoch: 0010 loss_train: 1.8925 acc_train: 0.1786 loss_val: 1.8851 acc_val: 0.1567 time: 0.0132s\n",
      "Epoch: 0011 loss_train: 1.8701 acc_train: 0.2071 loss_val: 1.8733 acc_val: 0.1567 time: 0.0137s\n",
      "Epoch: 0012 loss_train: 1.8566 acc_train: 0.1929 loss_val: 1.8622 acc_val: 0.1567 time: 0.0129s\n",
      "Epoch: 0013 loss_train: 1.8525 acc_train: 0.2000 loss_val: 1.8518 acc_val: 0.1567 time: 0.0142s\n",
      "Epoch: 0014 loss_train: 1.8371 acc_train: 0.1929 loss_val: 1.8419 acc_val: 0.1567 time: 0.0122s\n",
      "Epoch: 0015 loss_train: 1.8268 acc_train: 0.2000 loss_val: 1.8325 acc_val: 0.1567 time: 0.0123s\n",
      "Epoch: 0016 loss_train: 1.8226 acc_train: 0.2143 loss_val: 1.8235 acc_val: 0.1567 time: 0.0129s\n",
      "Epoch: 0017 loss_train: 1.8010 acc_train: 0.1929 loss_val: 1.8148 acc_val: 0.1567 time: 0.0127s\n",
      "Epoch: 0018 loss_train: 1.7972 acc_train: 0.2000 loss_val: 1.8065 acc_val: 0.1567 time: 0.0203s\n",
      "Epoch: 0019 loss_train: 1.7757 acc_train: 0.2286 loss_val: 1.7986 acc_val: 0.1567 time: 0.0150s\n",
      "Epoch: 0020 loss_train: 1.7966 acc_train: 0.1786 loss_val: 1.7908 acc_val: 0.1567 time: 0.0318s\n",
      "Epoch: 0021 loss_train: 1.7712 acc_train: 0.2643 loss_val: 1.7829 acc_val: 0.1567 time: 0.0137s\n",
      "Epoch: 0022 loss_train: 1.7664 acc_train: 0.3214 loss_val: 1.7753 acc_val: 0.2233 time: 0.0119s\n",
      "Epoch: 0023 loss_train: 1.7471 acc_train: 0.2857 loss_val: 1.7677 acc_val: 0.3467 time: 0.0117s\n",
      "Epoch: 0024 loss_train: 1.7640 acc_train: 0.2929 loss_val: 1.7601 acc_val: 0.3500 time: 0.0123s\n",
      "Epoch: 0025 loss_train: 1.7570 acc_train: 0.2929 loss_val: 1.7527 acc_val: 0.3500 time: 0.0131s\n",
      "Epoch: 0026 loss_train: 1.7373 acc_train: 0.3214 loss_val: 1.7453 acc_val: 0.3500 time: 0.0130s\n",
      "Epoch: 0027 loss_train: 1.7293 acc_train: 0.3143 loss_val: 1.7379 acc_val: 0.3500 time: 0.0106s\n",
      "Epoch: 0028 loss_train: 1.7509 acc_train: 0.2929 loss_val: 1.7305 acc_val: 0.3500 time: 0.0125s\n",
      "Epoch: 0029 loss_train: 1.7040 acc_train: 0.3000 loss_val: 1.7229 acc_val: 0.3500 time: 0.0132s\n",
      "Epoch: 0030 loss_train: 1.7022 acc_train: 0.2929 loss_val: 1.7153 acc_val: 0.3500 time: 0.0146s\n",
      "Epoch: 0031 loss_train: 1.7188 acc_train: 0.2929 loss_val: 1.7076 acc_val: 0.3500 time: 0.0231s\n",
      "Epoch: 0032 loss_train: 1.6741 acc_train: 0.3000 loss_val: 1.7001 acc_val: 0.3500 time: 0.0217s\n",
      "Epoch: 0033 loss_train: 1.6738 acc_train: 0.3071 loss_val: 1.6926 acc_val: 0.3500 time: 0.0171s\n",
      "Epoch: 0034 loss_train: 1.6824 acc_train: 0.3000 loss_val: 1.6847 acc_val: 0.3500 time: 0.0148s\n",
      "Epoch: 0035 loss_train: 1.6587 acc_train: 0.3000 loss_val: 1.6766 acc_val: 0.3467 time: 0.0124s\n",
      "Epoch: 0036 loss_train: 1.6580 acc_train: 0.3286 loss_val: 1.6685 acc_val: 0.3467 time: 0.0126s\n",
      "Epoch: 0037 loss_train: 1.6314 acc_train: 0.3071 loss_val: 1.6599 acc_val: 0.3467 time: 0.0139s\n",
      "Epoch: 0038 loss_train: 1.6433 acc_train: 0.3000 loss_val: 1.6512 acc_val: 0.3500 time: 0.0137s\n",
      "Epoch: 0039 loss_train: 1.6193 acc_train: 0.3286 loss_val: 1.6420 acc_val: 0.3567 time: 0.0135s\n",
      "Epoch: 0040 loss_train: 1.5867 acc_train: 0.3143 loss_val: 1.6329 acc_val: 0.3633 time: 0.0109s\n",
      "Epoch: 0041 loss_train: 1.5834 acc_train: 0.3071 loss_val: 1.6233 acc_val: 0.3733 time: 0.0128s\n",
      "Epoch: 0042 loss_train: 1.6064 acc_train: 0.3143 loss_val: 1.6133 acc_val: 0.3767 time: 0.0119s\n",
      "Epoch: 0043 loss_train: 1.5517 acc_train: 0.3643 loss_val: 1.6028 acc_val: 0.3967 time: 0.0118s\n",
      "Epoch: 0044 loss_train: 1.5243 acc_train: 0.3857 loss_val: 1.5923 acc_val: 0.4067 time: 0.0109s\n",
      "Epoch: 0045 loss_train: 1.5172 acc_train: 0.3929 loss_val: 1.5814 acc_val: 0.4167 time: 0.0138s\n",
      "Epoch: 0046 loss_train: 1.5225 acc_train: 0.3571 loss_val: 1.5698 acc_val: 0.4233 time: 0.0158s\n",
      "Epoch: 0047 loss_train: 1.5097 acc_train: 0.4000 loss_val: 1.5577 acc_val: 0.4233 time: 0.0159s\n",
      "Epoch: 0048 loss_train: 1.4857 acc_train: 0.4071 loss_val: 1.5452 acc_val: 0.4300 time: 0.0156s\n",
      "Epoch: 0049 loss_train: 1.4502 acc_train: 0.4143 loss_val: 1.5323 acc_val: 0.4300 time: 0.0148s\n",
      "Epoch: 0050 loss_train: 1.4427 acc_train: 0.4357 loss_val: 1.5192 acc_val: 0.4300 time: 0.0115s\n",
      "Epoch: 0051 loss_train: 1.4107 acc_train: 0.4571 loss_val: 1.5058 acc_val: 0.4333 time: 0.0111s\n",
      "Epoch: 0052 loss_train: 1.4033 acc_train: 0.4643 loss_val: 1.4926 acc_val: 0.4400 time: 0.0128s\n",
      "Epoch: 0053 loss_train: 1.4220 acc_train: 0.3929 loss_val: 1.4794 acc_val: 0.4467 time: 0.0115s\n",
      "Epoch: 0054 loss_train: 1.3874 acc_train: 0.5071 loss_val: 1.4663 acc_val: 0.4767 time: 0.0122s\n",
      "Epoch: 0055 loss_train: 1.3584 acc_train: 0.5000 loss_val: 1.4535 acc_val: 0.4833 time: 0.0108s\n",
      "Epoch: 0056 loss_train: 1.3352 acc_train: 0.5571 loss_val: 1.4406 acc_val: 0.4900 time: 0.0113s\n",
      "Epoch: 0057 loss_train: 1.3188 acc_train: 0.5857 loss_val: 1.4274 acc_val: 0.5000 time: 0.0128s\n",
      "Epoch: 0058 loss_train: 1.3310 acc_train: 0.5643 loss_val: 1.4142 acc_val: 0.5267 time: 0.0108s\n",
      "Epoch: 0059 loss_train: 1.2744 acc_train: 0.6071 loss_val: 1.4007 acc_val: 0.5500 time: 0.0119s\n",
      "Epoch: 0060 loss_train: 1.2412 acc_train: 0.6429 loss_val: 1.3865 acc_val: 0.5833 time: 0.0130s\n",
      "Epoch: 0061 loss_train: 1.2391 acc_train: 0.6500 loss_val: 1.3719 acc_val: 0.6033 time: 0.0128s\n",
      "Epoch: 0062 loss_train: 1.2053 acc_train: 0.6571 loss_val: 1.3572 acc_val: 0.6200 time: 0.0123s\n",
      "Epoch: 0063 loss_train: 1.1980 acc_train: 0.6929 loss_val: 1.3422 acc_val: 0.6267 time: 0.0186s\n",
      "Epoch: 0064 loss_train: 1.2038 acc_train: 0.6929 loss_val: 1.3269 acc_val: 0.6367 time: 0.0143s\n",
      "Epoch: 0065 loss_train: 1.2022 acc_train: 0.6929 loss_val: 1.3111 acc_val: 0.6400 time: 0.0157s\n",
      "Epoch: 0066 loss_train: 1.1191 acc_train: 0.7286 loss_val: 1.2954 acc_val: 0.6500 time: 0.0129s\n",
      "Epoch: 0067 loss_train: 1.1608 acc_train: 0.6929 loss_val: 1.2800 acc_val: 0.6500 time: 0.0131s\n",
      "Epoch: 0068 loss_train: 1.0895 acc_train: 0.7429 loss_val: 1.2652 acc_val: 0.6567 time: 0.0127s\n",
      "Epoch: 0069 loss_train: 1.1108 acc_train: 0.7286 loss_val: 1.2509 acc_val: 0.6700 time: 0.0136s\n",
      "Epoch: 0070 loss_train: 1.0929 acc_train: 0.7143 loss_val: 1.2373 acc_val: 0.6833 time: 0.0110s\n",
      "Epoch: 0071 loss_train: 1.0433 acc_train: 0.7571 loss_val: 1.2244 acc_val: 0.7000 time: 0.0162s\n",
      "Epoch: 0072 loss_train: 1.0831 acc_train: 0.7571 loss_val: 1.2117 acc_val: 0.7133 time: 0.0154s\n",
      "Epoch: 0073 loss_train: 1.0263 acc_train: 0.8214 loss_val: 1.1987 acc_val: 0.7333 time: 0.0117s\n",
      "Epoch: 0074 loss_train: 1.0433 acc_train: 0.7214 loss_val: 1.1849 acc_val: 0.7367 time: 0.0125s\n",
      "Epoch: 0075 loss_train: 1.0097 acc_train: 0.7929 loss_val: 1.1710 acc_val: 0.7433 time: 0.0111s\n",
      "Epoch: 0076 loss_train: 0.9614 acc_train: 0.8071 loss_val: 1.1565 acc_val: 0.7467 time: 0.0125s\n",
      "Epoch: 0077 loss_train: 0.9910 acc_train: 0.8214 loss_val: 1.1433 acc_val: 0.7500 time: 0.0180s\n",
      "Epoch: 0078 loss_train: 0.9531 acc_train: 0.8143 loss_val: 1.1307 acc_val: 0.7567 time: 0.0212s\n",
      "Epoch: 0079 loss_train: 0.9590 acc_train: 0.7929 loss_val: 1.1180 acc_val: 0.7800 time: 0.0184s\n",
      "Epoch: 0080 loss_train: 0.9260 acc_train: 0.8214 loss_val: 1.1056 acc_val: 0.7833 time: 0.0175s\n",
      "Epoch: 0081 loss_train: 0.9098 acc_train: 0.8143 loss_val: 1.0939 acc_val: 0.7867 time: 0.0138s\n",
      "Epoch: 0082 loss_train: 0.8808 acc_train: 0.8571 loss_val: 1.0822 acc_val: 0.7900 time: 0.0148s\n",
      "Epoch: 0083 loss_train: 0.8789 acc_train: 0.8143 loss_val: 1.0708 acc_val: 0.8000 time: 0.0139s\n",
      "Epoch: 0084 loss_train: 0.8770 acc_train: 0.8357 loss_val: 1.0595 acc_val: 0.8000 time: 0.0132s\n",
      "Epoch: 0085 loss_train: 0.8591 acc_train: 0.8286 loss_val: 1.0482 acc_val: 0.7833 time: 0.0158s\n",
      "Epoch: 0086 loss_train: 0.8598 acc_train: 0.8357 loss_val: 1.0367 acc_val: 0.7867 time: 0.0138s\n",
      "Epoch: 0087 loss_train: 0.8415 acc_train: 0.8357 loss_val: 1.0258 acc_val: 0.7933 time: 0.0137s\n",
      "Epoch: 0088 loss_train: 0.8301 acc_train: 0.8429 loss_val: 1.0155 acc_val: 0.7967 time: 0.0121s\n",
      "Epoch: 0089 loss_train: 0.7811 acc_train: 0.8714 loss_val: 1.0056 acc_val: 0.8033 time: 0.0132s\n",
      "Epoch: 0090 loss_train: 0.8235 acc_train: 0.8357 loss_val: 0.9966 acc_val: 0.7967 time: 0.0116s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0091 loss_train: 0.7830 acc_train: 0.8857 loss_val: 0.9872 acc_val: 0.8033 time: 0.0182s\n",
      "Epoch: 0092 loss_train: 0.7530 acc_train: 0.8429 loss_val: 0.9783 acc_val: 0.8067 time: 0.0180s\n",
      "Epoch: 0093 loss_train: 0.7653 acc_train: 0.8571 loss_val: 0.9696 acc_val: 0.8033 time: 0.0135s\n",
      "Epoch: 0094 loss_train: 0.7566 acc_train: 0.8571 loss_val: 0.9613 acc_val: 0.8033 time: 0.0147s\n",
      "Epoch: 0095 loss_train: 0.7487 acc_train: 0.8714 loss_val: 0.9527 acc_val: 0.8133 time: 0.0112s\n",
      "Epoch: 0096 loss_train: 0.7865 acc_train: 0.8500 loss_val: 0.9447 acc_val: 0.8067 time: 0.0145s\n",
      "Epoch: 0097 loss_train: 0.7473 acc_train: 0.8571 loss_val: 0.9368 acc_val: 0.8100 time: 0.0118s\n",
      "Epoch: 0098 loss_train: 0.7450 acc_train: 0.8143 loss_val: 0.9297 acc_val: 0.8067 time: 0.0143s\n",
      "Epoch: 0099 loss_train: 0.6852 acc_train: 0.8714 loss_val: 0.9226 acc_val: 0.8067 time: 0.0121s\n",
      "Epoch: 0100 loss_train: 0.6920 acc_train: 0.8929 loss_val: 0.9152 acc_val: 0.8100 time: 0.0135s\n",
      "Epoch: 0101 loss_train: 0.7378 acc_train: 0.8714 loss_val: 0.9083 acc_val: 0.8100 time: 0.0155s\n",
      "Epoch: 0102 loss_train: 0.7122 acc_train: 0.8500 loss_val: 0.9018 acc_val: 0.8100 time: 0.0133s\n",
      "Epoch: 0103 loss_train: 0.6495 acc_train: 0.8786 loss_val: 0.8959 acc_val: 0.8067 time: 0.0128s\n",
      "Epoch: 0104 loss_train: 0.6907 acc_train: 0.8571 loss_val: 0.8911 acc_val: 0.8100 time: 0.0113s\n",
      "Epoch: 0105 loss_train: 0.6782 acc_train: 0.8857 loss_val: 0.8866 acc_val: 0.8033 time: 0.0122s\n",
      "Epoch: 0106 loss_train: 0.6678 acc_train: 0.8714 loss_val: 0.8810 acc_val: 0.8033 time: 0.0160s\n",
      "Epoch: 0107 loss_train: 0.6560 acc_train: 0.8929 loss_val: 0.8750 acc_val: 0.8000 time: 0.0171s\n",
      "Epoch: 0108 loss_train: 0.6592 acc_train: 0.8857 loss_val: 0.8687 acc_val: 0.8000 time: 0.0132s\n",
      "Epoch: 0109 loss_train: 0.6536 acc_train: 0.8643 loss_val: 0.8632 acc_val: 0.7967 time: 0.0151s\n",
      "Epoch: 0110 loss_train: 0.6392 acc_train: 0.9000 loss_val: 0.8574 acc_val: 0.8067 time: 0.0157s\n",
      "Epoch: 0111 loss_train: 0.6486 acc_train: 0.8714 loss_val: 0.8526 acc_val: 0.8067 time: 0.0129s\n",
      "Epoch: 0112 loss_train: 0.5919 acc_train: 0.8857 loss_val: 0.8485 acc_val: 0.8033 time: 0.0127s\n",
      "Epoch: 0113 loss_train: 0.6247 acc_train: 0.8714 loss_val: 0.8452 acc_val: 0.8000 time: 0.0113s\n",
      "Epoch: 0114 loss_train: 0.6221 acc_train: 0.8643 loss_val: 0.8411 acc_val: 0.8000 time: 0.0135s\n",
      "Epoch: 0115 loss_train: 0.5779 acc_train: 0.8929 loss_val: 0.8361 acc_val: 0.8000 time: 0.0123s\n",
      "Epoch: 0116 loss_train: 0.6401 acc_train: 0.8429 loss_val: 0.8310 acc_val: 0.8033 time: 0.0131s\n",
      "Epoch: 0117 loss_train: 0.5767 acc_train: 0.9071 loss_val: 0.8256 acc_val: 0.8033 time: 0.1109s\n",
      "Epoch: 0118 loss_train: 0.5608 acc_train: 0.8929 loss_val: 0.8212 acc_val: 0.8033 time: 0.0178s\n",
      "Epoch: 0119 loss_train: 0.5949 acc_train: 0.8714 loss_val: 0.8177 acc_val: 0.8067 time: 0.0134s\n",
      "Epoch: 0120 loss_train: 0.5988 acc_train: 0.8714 loss_val: 0.8139 acc_val: 0.8067 time: 0.0138s\n",
      "Epoch: 0121 loss_train: 0.5654 acc_train: 0.8857 loss_val: 0.8101 acc_val: 0.8133 time: 0.0131s\n",
      "Epoch: 0122 loss_train: 0.5989 acc_train: 0.8714 loss_val: 0.8062 acc_val: 0.8167 time: 0.0136s\n",
      "Epoch: 0123 loss_train: 0.5618 acc_train: 0.8929 loss_val: 0.8018 acc_val: 0.8200 time: 0.0117s\n",
      "Epoch: 0124 loss_train: 0.5484 acc_train: 0.8929 loss_val: 0.7973 acc_val: 0.8233 time: 0.0181s\n",
      "Epoch: 0125 loss_train: 0.5731 acc_train: 0.8786 loss_val: 0.7940 acc_val: 0.8200 time: 0.0183s\n",
      "Epoch: 0126 loss_train: 0.5336 acc_train: 0.8929 loss_val: 0.7908 acc_val: 0.8233 time: 0.0132s\n",
      "Epoch: 0127 loss_train: 0.5280 acc_train: 0.8857 loss_val: 0.7891 acc_val: 0.8133 time: 0.0144s\n",
      "Epoch: 0128 loss_train: 0.5666 acc_train: 0.9071 loss_val: 0.7888 acc_val: 0.8067 time: 0.0122s\n",
      "Epoch: 0129 loss_train: 0.5679 acc_train: 0.8929 loss_val: 0.7887 acc_val: 0.8133 time: 0.0130s\n",
      "Epoch: 0130 loss_train: 0.5557 acc_train: 0.9071 loss_val: 0.7893 acc_val: 0.8000 time: 0.0117s\n",
      "Epoch: 0131 loss_train: 0.5439 acc_train: 0.9214 loss_val: 0.7892 acc_val: 0.8000 time: 0.0325s\n",
      "Epoch: 0132 loss_train: 0.5648 acc_train: 0.9000 loss_val: 0.7870 acc_val: 0.8000 time: 0.0192s\n",
      "Epoch: 0133 loss_train: 0.5445 acc_train: 0.9071 loss_val: 0.7844 acc_val: 0.8000 time: 0.0299s\n",
      "Epoch: 0134 loss_train: 0.5412 acc_train: 0.9071 loss_val: 0.7806 acc_val: 0.8000 time: 0.0148s\n",
      "Epoch: 0135 loss_train: 0.5165 acc_train: 0.9214 loss_val: 0.7746 acc_val: 0.8067 time: 0.0138s\n",
      "Epoch: 0136 loss_train: 0.5327 acc_train: 0.9071 loss_val: 0.7699 acc_val: 0.8100 time: 0.0137s\n",
      "Epoch: 0137 loss_train: 0.5173 acc_train: 0.9286 loss_val: 0.7657 acc_val: 0.8200 time: 0.0140s\n",
      "Epoch: 0138 loss_train: 0.4701 acc_train: 0.9214 loss_val: 0.7621 acc_val: 0.8300 time: 0.0129s\n",
      "Epoch: 0139 loss_train: 0.5142 acc_train: 0.9214 loss_val: 0.7592 acc_val: 0.8300 time: 0.0177s\n",
      "Epoch: 0140 loss_train: 0.5303 acc_train: 0.8929 loss_val: 0.7571 acc_val: 0.8267 time: 0.0152s\n",
      "Epoch: 0141 loss_train: 0.4905 acc_train: 0.9143 loss_val: 0.7558 acc_val: 0.8200 time: 0.0151s\n",
      "Epoch: 0142 loss_train: 0.5369 acc_train: 0.8857 loss_val: 0.7561 acc_val: 0.8067 time: 0.0139s\n",
      "Epoch: 0143 loss_train: 0.5125 acc_train: 0.9143 loss_val: 0.7573 acc_val: 0.8067 time: 0.0119s\n",
      "Epoch: 0144 loss_train: 0.4841 acc_train: 0.9071 loss_val: 0.7576 acc_val: 0.8100 time: 0.0167s\n",
      "Epoch: 0145 loss_train: 0.4867 acc_train: 0.9071 loss_val: 0.7566 acc_val: 0.8100 time: 0.0141s\n",
      "Epoch: 0146 loss_train: 0.5030 acc_train: 0.8929 loss_val: 0.7537 acc_val: 0.8100 time: 0.0121s\n",
      "Epoch: 0147 loss_train: 0.4931 acc_train: 0.8857 loss_val: 0.7505 acc_val: 0.8100 time: 0.0130s\n",
      "Epoch: 0148 loss_train: 0.5034 acc_train: 0.9000 loss_val: 0.7468 acc_val: 0.8067 time: 0.0105s\n",
      "Epoch: 0149 loss_train: 0.4762 acc_train: 0.9143 loss_val: 0.7430 acc_val: 0.8100 time: 0.0128s\n",
      "Epoch: 0150 loss_train: 0.5013 acc_train: 0.9286 loss_val: 0.7397 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0151 loss_train: 0.4780 acc_train: 0.9214 loss_val: 0.7372 acc_val: 0.8133 time: 0.0103s\n",
      "Epoch: 0152 loss_train: 0.4701 acc_train: 0.9214 loss_val: 0.7351 acc_val: 0.8200 time: 0.0120s\n",
      "Epoch: 0153 loss_train: 0.4800 acc_train: 0.9357 loss_val: 0.7341 acc_val: 0.8100 time: 0.0113s\n",
      "Epoch: 0154 loss_train: 0.4300 acc_train: 0.9286 loss_val: 0.7333 acc_val: 0.8100 time: 0.0113s\n",
      "Epoch: 0155 loss_train: 0.4648 acc_train: 0.9357 loss_val: 0.7318 acc_val: 0.8067 time: 0.0124s\n",
      "Epoch: 0156 loss_train: 0.4328 acc_train: 0.9286 loss_val: 0.7298 acc_val: 0.8067 time: 0.0132s\n",
      "Epoch: 0157 loss_train: 0.4689 acc_train: 0.9286 loss_val: 0.7284 acc_val: 0.8167 time: 0.0125s\n",
      "Epoch: 0158 loss_train: 0.4394 acc_train: 0.9286 loss_val: 0.7272 acc_val: 0.8133 time: 0.0119s\n",
      "Epoch: 0159 loss_train: 0.4495 acc_train: 0.9214 loss_val: 0.7269 acc_val: 0.8067 time: 0.0118s\n",
      "Epoch: 0160 loss_train: 0.4393 acc_train: 0.9500 loss_val: 0.7272 acc_val: 0.8033 time: 0.0140s\n",
      "Epoch: 0161 loss_train: 0.4717 acc_train: 0.9071 loss_val: 0.7265 acc_val: 0.8067 time: 0.0190s\n",
      "Epoch: 0162 loss_train: 0.4265 acc_train: 0.9571 loss_val: 0.7254 acc_val: 0.8067 time: 0.0190s\n",
      "Epoch: 0163 loss_train: 0.4443 acc_train: 0.9357 loss_val: 0.7230 acc_val: 0.8067 time: 0.0132s\n",
      "Epoch: 0164 loss_train: 0.4482 acc_train: 0.9500 loss_val: 0.7211 acc_val: 0.8033 time: 0.0156s\n",
      "Epoch: 0165 loss_train: 0.4281 acc_train: 0.9357 loss_val: 0.7200 acc_val: 0.8033 time: 0.0123s\n",
      "Epoch: 0166 loss_train: 0.4131 acc_train: 0.9500 loss_val: 0.7174 acc_val: 0.8200 time: 0.0139s\n",
      "Epoch: 0167 loss_train: 0.4293 acc_train: 0.9214 loss_val: 0.7150 acc_val: 0.8200 time: 0.0146s\n",
      "Epoch: 0168 loss_train: 0.4403 acc_train: 0.9143 loss_val: 0.7136 acc_val: 0.8100 time: 0.0124s\n",
      "Epoch: 0169 loss_train: 0.4457 acc_train: 0.9500 loss_val: 0.7119 acc_val: 0.8067 time: 0.0123s\n",
      "Epoch: 0170 loss_train: 0.4218 acc_train: 0.9571 loss_val: 0.7111 acc_val: 0.8067 time: 0.0117s\n",
      "Epoch: 0171 loss_train: 0.4393 acc_train: 0.9214 loss_val: 0.7105 acc_val: 0.8067 time: 0.0133s\n",
      "Epoch: 0172 loss_train: 0.4150 acc_train: 0.9286 loss_val: 0.7083 acc_val: 0.8067 time: 0.0138s\n",
      "Epoch: 0173 loss_train: 0.4174 acc_train: 0.9357 loss_val: 0.7061 acc_val: 0.8100 time: 0.0116s\n",
      "Epoch: 0174 loss_train: 0.4729 acc_train: 0.9071 loss_val: 0.7032 acc_val: 0.8167 time: 0.0122s\n",
      "Epoch: 0175 loss_train: 0.4225 acc_train: 0.9214 loss_val: 0.7013 acc_val: 0.8233 time: 0.0114s\n",
      "Epoch: 0176 loss_train: 0.4079 acc_train: 0.9571 loss_val: 0.7005 acc_val: 0.8300 time: 0.0144s\n",
      "Epoch: 0177 loss_train: 0.4669 acc_train: 0.9214 loss_val: 0.6993 acc_val: 0.8233 time: 0.0171s\n",
      "Epoch: 0178 loss_train: 0.4298 acc_train: 0.9286 loss_val: 0.6987 acc_val: 0.8200 time: 0.0116s\n",
      "Epoch: 0179 loss_train: 0.4183 acc_train: 0.9429 loss_val: 0.7001 acc_val: 0.8167 time: 0.0128s\n",
      "Epoch: 0180 loss_train: 0.3952 acc_train: 0.9571 loss_val: 0.7017 acc_val: 0.8100 time: 0.0109s\n",
      "Epoch: 0181 loss_train: 0.4258 acc_train: 0.9429 loss_val: 0.7015 acc_val: 0.8100 time: 0.0113s\n",
      "Epoch: 0182 loss_train: 0.4021 acc_train: 0.9500 loss_val: 0.7005 acc_val: 0.8100 time: 0.0125s\n",
      "Epoch: 0183 loss_train: 0.4356 acc_train: 0.9357 loss_val: 0.6987 acc_val: 0.8167 time: 0.0130s\n",
      "Epoch: 0184 loss_train: 0.4060 acc_train: 0.9500 loss_val: 0.6960 acc_val: 0.8167 time: 0.0126s\n",
      "Epoch: 0185 loss_train: 0.4114 acc_train: 0.9500 loss_val: 0.6945 acc_val: 0.8167 time: 0.0126s\n",
      "Epoch: 0186 loss_train: 0.4221 acc_train: 0.9571 loss_val: 0.6922 acc_val: 0.8167 time: 0.0118s\n",
      "Epoch: 0187 loss_train: 0.4205 acc_train: 0.9214 loss_val: 0.6886 acc_val: 0.8333 time: 0.0149s\n",
      "Epoch: 0188 loss_train: 0.3850 acc_train: 0.9214 loss_val: 0.6853 acc_val: 0.8333 time: 0.0151s\n",
      "Epoch: 0189 loss_train: 0.3904 acc_train: 0.9643 loss_val: 0.6833 acc_val: 0.8333 time: 0.0166s\n",
      "Epoch: 0190 loss_train: 0.4050 acc_train: 0.9286 loss_val: 0.6825 acc_val: 0.8200 time: 0.0120s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0191 loss_train: 0.3677 acc_train: 0.9500 loss_val: 0.6822 acc_val: 0.8200 time: 0.0123s\n",
      "Epoch: 0192 loss_train: 0.3724 acc_train: 0.9429 loss_val: 0.6831 acc_val: 0.8167 time: 0.0173s\n",
      "Epoch: 0193 loss_train: 0.4101 acc_train: 0.9214 loss_val: 0.6852 acc_val: 0.8167 time: 0.0157s\n",
      "Epoch: 0194 loss_train: 0.3706 acc_train: 0.9571 loss_val: 0.6859 acc_val: 0.8167 time: 0.0197s\n",
      "Epoch: 0195 loss_train: 0.4031 acc_train: 0.9357 loss_val: 0.6868 acc_val: 0.8167 time: 0.0161s\n",
      "Epoch: 0196 loss_train: 0.3844 acc_train: 0.9286 loss_val: 0.6871 acc_val: 0.8167 time: 0.0155s\n",
      "Epoch: 0197 loss_train: 0.3763 acc_train: 0.9571 loss_val: 0.6875 acc_val: 0.8167 time: 0.0131s\n",
      "Epoch: 0198 loss_train: 0.4031 acc_train: 0.9357 loss_val: 0.6890 acc_val: 0.8167 time: 0.0169s\n",
      "Epoch: 0199 loss_train: 0.4045 acc_train: 0.9500 loss_val: 0.6893 acc_val: 0.8233 time: 0.0212s\n",
      "Epoch: 0200 loss_train: 0.4001 acc_train: 0.9571 loss_val: 0.6881 acc_val: 0.8200 time: 0.0164s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 3.0167s\n",
      "Test set results: loss= 0.7138 accuracy= 0.8260\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "# epoch数\n",
    "for epoch in range(args.epochs):\n",
    "    # 训练\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "# 已用总时间\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
